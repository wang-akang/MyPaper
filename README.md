### å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼š
1. [2023 ICCV] **PatchCT: Aligning Patch Set and Label Set with Conditional Transport
for Multi-Label Image Classification**[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PatchCT_Aligning_Patch_Set_and_Label_Set_with_Conditional_Transport_ICCV_2023_paper.pdf)
2. [2023 ICCV] **Cdul: Clip-driven unsupervised learning for multi-label image classification**[[paper]](https://arxiv.org/pdf/2307.16634)[[code]](https://github.com/cs-mshah/CDUL)
3. [2024 ICML] **Language-driven Cross-modal Classifier for
Zero-shot Multi-label Image Recognition**[[paper]](https://openreview.net/pdf?id=sHswzNWUW2)[[code]](https://github.com/yic20/CoMC)
4. [2024 AAAI] **TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training**[[paper]](https://arxiv.org/pdf/2312.12828)[[code]](https://github.com/linyq2117/TagCLIP)
5. [2025 CVPR] **SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models**[[paper]](https://arxiv.org/pdf/2502.16911?)[[code]](https://github.com/kjmillerCURIS/SPARC)
6. [2025 CVPR] **Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification**[[paper]](https://arxiv.org/pdf/2503.16873)[[code]](https://github.com/k0u-id/CCD)
7. [2025 CVPR] **Recover and Match: Open-Vocabulary Multi-Label Recognition through
Knowledge-Constrained Optimal Transport**[[paper]](https://arxiv.org/pdf/2503.15337)[[code]](https://github.com/EricTan7/RAM)
8. [2025 CVPR] **Correlative and Discriminative Label Grouping for Multi-Label
Visual Prompt Tuning**[[paper]](https://arxiv.org/pdf/2504.09990)
9. [2025 ICML] **From Local Details to Global Context:Advancing Vision-Language Models with Attention-Based Selection**[[paper]](https://arxiv.org/pdf/2505.13233?)[[code]](https://github.com/BIT-DA/ABS)
10. [2025 WACV] **Pay Attention to Your Neighbours:Training-Free Open-Vocabulary Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2404.08181?)[[code]](https://github.com/sinahmr/NACLIP)
11. [2025 ICCV] **Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification**
12. [unknown] **Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation**[[paper]](https://arxiv.org/pdf/2411.15869)[[code]](https://github.com/SuleBai/SC-CLIP)
13. [2025 ICCV]MambaML: Exploring State Space Models for Multi-Label Image Classification
14. [2025 ICCV]Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification
15. [2025 ICCV] **More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning**[[paper]](https://arxiv.org/pdf/2508.20381)
16. [2025 ICCV]Language-Driven Multi-Label Zero-Shot Learning with Semantic Granularity
17. [2024 ICLR] **A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation**[[paper]](https://arxiv.org/pdf/2402.04087)[[code]](https://github.com/mrflogs/ICLR24)
>æ— éœ€è®­ç»ƒâ€çš„CLIPæ¨¡å‹è‡ªé€‚åº”æ–°æ–¹æ³•
18. [2022 IJCV] **Learning to Prompt for Vision-Language Models**[[paper]](https://arxiv.org/pdf/2109.01134v3)[[code]](https://github.com/KaiyangZhou/CoOp)
19. [2025 CVPR] **DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception**[[paper]](https://arxiv.org/pdf/2505.04410)[[code]](https://github.com/xiaomoguhz/DeCLIP)
>DeCLIP é€šè¿‡è§£è€¦è‡ªæ³¨æ„åŠ›æ¨¡å—å¹¶åˆ†åˆ«å¯¹å†…å®¹ç‰¹å¾å’Œä¸Šä¸‹æ–‡ç‰¹å¾è¿›è¡Œè’¸é¦ï¼Œæœ‰æ•ˆåœ°è§£å†³äº† CLIP åœ¨å¼€æ”¾è¯æ±‡å¯†é›†æ„ŸçŸ¥ä»»åŠ¡ä¸­å±€éƒ¨ç‰¹å¾è¾¨åˆ«åŠ›ä¸è¶³å’Œç©ºé—´ä¸€è‡´æ€§å·®çš„é—®é¢˜ã€‚
### çº¿æ€§ç‰¹å¾å¯¹é½ï¼š
1. [2025 ICCV]**Black Box Few-Shot Adaptation for Vision-Language models**[[paper]](https://arxiv.org/pdf/2304.01752v3)[[code]](https://github.com/saic-fi/LFA)

### è¯­ä¹‰åˆ†å‰²:
1. [2025 CVPR] **Test-Time Adaptation of Vision-Language Models forOpen-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2505.21844v1)[[code]](https://github.com/dosowiechi/MLMP)
2. [2025 ICCV] **Optimal Transport-assisted Proxy Learning for Weakly Supervised Semantic Segmentation**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/1933)
3. [2025 ICCV] **Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/html/2507.06848v1)
4. [2025 ICCV] **Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/645)
5. [2025 NIPS] **Disentangling CLIP for Multi-Object Perception**[[paper]](https://arxiv.org/pdf/2502.02977v3)
>VLMs ä¸­ç±»åˆ«ç‰¹å¾ä¹‹é—´çš„é«˜ MFI ä¸¥é‡å½±å“äº†å…¶å¤šå¯¹è±¡æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ MFI Loss(è§£è€¦æ–‡æœ¬ç‰¹å¾) å’Œ ASL Loss æ¥è§£è€¦ CLIP ç‰¹å¾è®­ç»ƒç›®æ ‡é™ä½æ€»è®­ç»ƒæŸå¤±æ˜¯ MFI æŸå¤±å’Œ ASL æŸå¤±çš„ç»„åˆ
6. [2023 ICCV] **Zero-guidance Segmentation Using Zero Segment Labels**[[paper]](https://arxiv.org/pdf/2303.13396)
>DINO-ViT æ¨¡å‹æå–å›¾åƒçš„æ·±å±‚åƒç´ çº§ç‰¹å¾ï¼Œèšç±»å¾—åˆ°åˆ†å‰²æ©ç ï¼Œè¾“å…¥å›¾åƒå’Œé€šè¿‡èšç±»å¾—åˆ°çš„äºŒå€¼æ©ç ä¼šè¢«åŒæ—¶é€å…¥CLIPï¼Œæ–°é¢–çš„æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskingï¼‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å…¨å±€æ¶ˆå‡ï¼ˆGlobal Subtractionï¼‰ï¼Œä½¿ç”¨ ZeroCap æŠŠå›¾åƒç”Ÿæˆæ–‡æœ¬ï¼Œç›¸ä¼¼åº¦åˆ†æ•°æ˜¯è§†è§‰åµŒå…¥å’Œé¢„æµ‹æ–‡æœ¬åµŒå…¥ï¼ˆé€šè¿‡ CLIP æ–‡æœ¬ç¼–ç å™¨è®¡ç®—ï¼‰çš„ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
7. [2025 ICCV] **Enhancing Few-Shot Vision-Language Classification with Large MultimodalModel Features**[[paper]](https://arxiv.org/pdf/2412.00142)
8. [2025 CVPR] **Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model**[[paper]](https://arxiv.org/pdf/2503.16282)
9. [2025 ICCV] **DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary**[[paper]](https://www.arxiv.org/pdf/2508.13560)[[code]](https://github.com/xiaozhen228/DictAS)
10. [2025 arXiv] **No time to train! Training-Free Reference-Based Instance Segmentation**[[paper]](https://arxiv.org/pdf/2507.02798)[[code]](https://github.com/miquel-espinosa/no-time-to-train) ğŸ‘Œ
>DINOv2ä¸SAM2ç»“åˆå°‘æ ·æœ¬åˆ†å‰²
12. [2025 ICCV] **DenseVLM: A Retrieval and Decoupled Alignment Framework for Open-Vocabulary Dense Prediction**[[paper]](https://arxiv.org/pdf/2412.06244)[[code]](https://link.zhihu.com/?target=https%3A//github.com/HVision-NKU/DenseVLM)
>è§£å†³å‰æ™¯åå·®é—®é¢˜
13. [2025 arXiv] **TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models**[[paper]](https://arxiv.org/pdf/2505.23769v1)[[code]](https://github.com/avaxiao/TextRegion)
>SAM2ä¸å›¾åƒ-æ–‡æœ¬æ¨¡å‹(CLIP SigLIP2)ç»“åˆ
14. [2025 ICCV] **CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2411.10086)[[code]](https://github.com/zdk258/CorrCLIP)
>CLIPä¸SAMç»“åˆCorrCLIP åˆ©ç”¨åˆ†æ®µä»»æ„æ¨¡å‹ ï¼ˆSAMï¼‰ æ¥å®šä¹‰è¡¥ä¸äº¤äº’çš„èŒƒå›´ï¼Œä»è€Œå‡å°‘ç±»é—´ç›¸å…³æ€§
15. [2024 CVPR] **Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation**[[paper]](https://arxiv.org/pdf/2404.06542)[[code]](https://github.com/aimagelab/freeda/)
> **ç¦»çº¿åŸå‹ç”Ÿæˆé˜¶æ®µ**:ä½¿ç”¨ Stable Diffusion æ¨¡å‹,ç»“åˆå¤§é‡çš„æ–‡æœ¬æè¿°æå–å±€éƒ¨åŒ–æ©ç ,é‡‡ç”¨ DINOv2è§†è§‰åŸå‹æå–,CLIP æ–‡æœ¬é”®æå–,æ¯ä¸ªæ–‡æœ¬é”®éƒ½ä¸ä¸€ä¸ªè§†è§‰åŸå‹ç›¸å…³è”,æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„æ–‡æœ¬-è§†è§‰åŸå‹é›†åˆ
 **æ— è®­ç»ƒæ©ç é¢„æµ‹é˜¶æ®µ**:ç»™å®šä¸€ç»„æ–‡æœ¬ç±»åˆ«,æ£€ç´¢åˆ°çš„åŸå‹å–å¹³å‡å¾—åˆ°è§†è§‰åŸå‹.è¶…åƒç´ çš„å±€éƒ¨åŒºåŸŸåˆ†å‰²Felzenszwalb,CLIPçš„å…¨å±€ç›¸ä¼¼æ€§åŠ æƒ.
16. [2025 NeurIPS] **SANSA: Unleashing the Hidden Semantics in SAM2for Few-Shot Segmentation**[[paper]](https://arxiv.org/pdf/2505.21795)[[code]](https://github.com/ClaudiaCuttano/SANSA)
17. [2025 arXiv] **X-SAM: From Segment Anything to Any Segmentation**[[paper]](https://arxiv.org/pdf/2508.04655)[[code]](https://github.com/wanghao9610/X-SAM)
18. [2025 CVPR] **Segment Any Motion in Videos**[[paper]](https://arxiv.org/pdf/2503.22268)[[code]](https://github.com/nnanhuang/SegAnyMo)
19. [2025 NeurIPS] **OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts**[[paper]](https://arxiv.org/pdf/2507.05427)
20. [2025 ICCV] **Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation**[[paper]](https://arxiv.org/pdf/2411.19331)[[code]](https://lorebianchi98.github.io/Talk2DINO/)
21. [2025 ICIP] **Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2505.19846)
>ä½¿ç”¨SAMå’ŒCLIPé€šè¿‡zero-shotæ ‡æ³¨ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶é€šè¿‡UniMatchçš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ä½œä¸ºå¢å¼ºæ ‡ç­¾æ¥æé«˜å…¶è´¨é‡ã€‚


### æ£€ç´¢:
1. [2024 ICML] **Cluster-Aware Similarity Diffusion for Instance Retrieval**[[paper]](https://arxiv.org/pdf/2406.02343)
2. [2025 CVPR] **Cheb-GR: Rethinking k-nearest neighbor search in Re-ranking for Person
 Re-identification**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.pdf)
3. [2025 NEIGHBOR] **Neighbor-aware Geodesic Transportation for Neighborhood Refinery**[[paper]](https://openreview.net/pdf?id=DWI1xx2sX5)
4. [2021 NIPS] **Contextual Similarity Aggregation with Self-attention for Visual Re-ranking**[[paper]](https://arxiv.org/pdf/2110.13430)
5. [2027 AAAI] **Regularized diffusion process for visual retrieval**
6. [2025 arXiv] **Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking**[[paper]](https://arxiv.org/pdf/2509.04351)




### few-shotï¼š
1. [2025 ICCV] **Object-level Correlation for Few-Shot Segmentation**[[paper]](https://arxiv.org/pdf/2509.07917)
2. [2025 ICCV] **When Pixel Difference Patterns Meet ViT: PiDiViT for Few-Shot Object Detection**
3. [2025 ICCV] **Probabilistic Prototype Calibration of Vision-language Models for Generalized Few-shot Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2506.22979)[[code]](https://github.com/jliu4ai/FewCLIP)
4. [2025 ICCV] **Few-Shot Pattern Detection via Template Matching and Regression**[[paper]](https://arxiv.org/pdf/2508.17636)
5. [2025 ICCV] **Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition**
6. [2025 ICCV] **Text Augmented Correlation Transformer For Few-shot Classification & Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.pdf)
7. [2025 CVPR] **UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning**[[paper]](https://arxiv.org/pdf/2412.16739)[[code]](https://github.com/ZhouLong0/UNEM-Transductive)


### Training-Freeï¼š
1. [2024 CVPR] **Clip-diy: Clip dense inference yields open-vocabulary semantic segmentation for-free** [[paper]](https://arxiv.org/pdf/2309.14289)[[code]](https://github.com/wysoczanska/clip-diy)
2. [2024 CVPR] **Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation** [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10655445&tag=1) [[code]](https://github.com/aimagelab/freeda)
3. [2024 ECCV] **Diffusion Models for Open-Vocabulary Segmentation** [[paper]](https://arxiv.org/pdf/2306.09316) [[code]](https://github.com/karazijal/ovdiff)
4. [2024 ECCV] **ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference** [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06346.pdf)ğŸ‘Œ [[code]](https://github.com/mc-lan/ClearCLIP)
>CLIPäº§ç”Ÿäº†å…·æœ‰é”™è¯¯åˆ†å‰²åŒºåŸŸçš„å˜ˆæ‚åˆ†å‰²å›¾ï¼Œå»é™¤æ®‹å·®è¿æ¥ã€å®ç°è‡ªæ³¨æ„åŠ›å’Œä¸¢å¼ƒå‰é¦ˆç½‘ç»œã€‚ClearCLIP å§‹ç»ˆå¦‚ä¸€åœ°ç”Ÿæˆæ›´æ¸…æ™°ã€æ›´å‡†ç¡®çš„åˆ†å‰²å›¾
5. [2024 ECCV] **SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference** [[paper]](https://arxiv.org/pdf/2312.01597)ğŸ‘Œ [[code]](https://github.com/wangf3014/SCLIP)
>CLIPçš„åˆ†å‰²æ€§èƒ½ä¸ä½³æ˜¯ç”±æ–‘å—è¡¨ç¤ºçš„ç©ºé—´é”™ä½å¼•èµ·çš„ï¼Œè€Œä¸æ˜¯æ— æ³•æå–å¯†é›†çš„è§†è§‰ç‰¹å¾,é—®é¢˜å‡ºåœ¨CLIPçš„è‡ªæ³¨æ„åŠ›æ¨¡å—,ä½¿ç”¨ CSA æ¨¡å—ä»£æ›¿ CLIP è§†è§‰ç¼–ç å™¨ä¸­çš„åŸå§‹è‡ªæ³¨æ„åŠ›å—
6. [2024 ECCV] **Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2404.08181) [[code]](https://github.com/sinahmr/NACLIP)
7. [2024 ECCV] **Proxyclip: Proxy attention improves clip for open-vocabulary segmentation** [[paper]](https://arxiv.org/pdf/2408.04883) [[code]](https://github.com/mc-lan/ProxyCLIP?tab=readme-ov-file) 
8. [2024 ECCV] **Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2407.08268) [[code]](https://github.com/leaves162/CLIPtrase)
9. [2024 ICLR] **A Hard-to-Beat Baseline for Training-free CLIP-Based Adaptation** [[paper]](https://openreview.net/forum?id=Js5PJPHDyY) [[code]](https://github.com/mrflogs/ICLR24)
10. [2024 arXiv] **CLIPer: Hierarchically Improving Spatial Representation of CLIP for Open-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2411.13836) [[code]](https://github.com/linsun449/cliper.code?tab=readme-ov-file)
11. [2025 CVPR] **LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2503.19777) [[code]](https://github.com/vladan-stojnic/LPOSS)
12. [2025 CVPR] **ResCLIP: Residual Attention for Training-free Dense Vision-language Inference** [[paper]](https://arxiv.org/pdf/2411.15851)ğŸ‘Œ [[code]](https://github.com/yvhangyang/ResCLIP?tab=readme-ov-file)
>æ®‹å·®äº’ç›¸å…³è‡ªæ³¨æ„åŠ› ï¼ˆRCSï¼‰ å’Œè¯­ä¹‰åé¦ˆç»†åŒ– ï¼ˆSFRï¼‰ æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—å¯ä»¥çº æ­£æœ€åä¸€å±‚çš„æ³¨æ„åŠ›ï¼Œä»¥æ•è·ç‰¹å®šç±»çš„ç‰¹å¾å’Œå±€éƒ¨ä¸€è‡´æ€§ï¼Œä»è€Œæ”¹è¿›å¯†é›†è§†è§‰è¯­è¨€é¢„æµ‹ä»»åŠ¡çš„CLIPæ¨¡å‹ã€‚
13. [2025 CVPR] **Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Distilling_Spectral_Graph_for_Object-Context_Aware_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.pdf) [[code]](https://github.com/MICV-yonsei/CASS)
14. [2025 CVPR] **Cheb-GR: Rethinking k-nearest neighbor search in Re-ranking for Person Re-identification** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.pdf) [[code]](https://github.com/Jinxi-Yang-WHU/Fast-GCR.git) [[note]](æœ¬æ–‡æåˆ°çš„å¾ˆå¤šre-rankingçš„æŠ€æœ¯å°±æ˜¯å¯¹ç›´æ¥è®¡ç®—çš„ç›¸ä¼¼åº¦çŸ©é˜µè¿›è¡Œæ›´æ–°ï¼Œå‰é¢å…¬å¼æäº†ä¸€å¤§å †ï¼Œæœ€åå°±æ˜¯ä¸€ä¸ªç‰¹å¾ä¼ æ’­ã€‚)
15. [2025 CVPR] **ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements** [[paper]](https://openaccess.thecvf.com/content/CVPR2025W/PixFoundation/papers/Aydin_ITACLIP_Boosting_Training-Free_Semantic_Segmentation_with_Image_Text_and_Architectural_CVPRW_2025_paper.pdf) [[code]](https://github.com/m-arda-aydn/ITACLIP)
16. [2025 CVPR] **Search and Detect: Training-Free Long Tail Object Detection via Web-Image Retrieval** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Sidhu_Search_and_Detect_Training-Free_Long_Tail_Object_Detection_via_Web-Image_CVPR_2025_paper.pdf) [[code]](https://github.com/Mankeerat/SearchDet)
17. [2025 ICCV] **LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scene** [[paper]](https://arxiv.org/pdf/2410.14462#page=17.85) [[code]](https://github.com/naver/ludvig)
18. [2025 ICCV] **WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images** [[paper]](https://arxiv.org/pdf/2503.08407)[[code]](https://github.com/Ethan16162/WildSeg3D)
19. [2025 ICCV] **Harnessing Vision Foundation Models for High-Performance, Training-Free Open Vocabulary Segmentation** [[paper]](https://arxiv.org/pdf/2411.09219) [[code]](https://github.com/YuHengsss/Trident)
20. [2025 ICCV] **E-SAM: Training-Free Segment Every Entity Model** [[paper]](https://arxiv.org/pdf/2503.12094)
21. [2025 ICCV] **ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation** [[paper]](https://arxiv.org/pdf/2506.21233) [[code]](https://github.com/xiweix/ReME)
22. [2025 ICCV] **CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting** [[paper]](https://arxiv.org/pdf/2505.20469) [[code]](https://epsilontl.github.io/CCL-LGS/)
23. [2025 ICCV] **Auto-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2312.04539)[[code]](https://github.com/ozzyou/AutoSeg)
24. [2025 ICCV] **Understanding Personal Concept in Open-Vocabulary Semantic Segmentation**
25. [2025 ICCV] **Training-Free Class Purification for Open-Vocabulary Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2508.00557)
26. [2025 ICCV] **DIH-CLIP: Unleashing the Diversity of Multi-Head Self-Attention for Training-Free Open-Vocabulary Semantic Segmentation**
27. [2025 ICCV] **Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild**[[paper]](https://arxiv.org/pdf/2508.07759)[[code]](https://github.com/wanghr64/cav-sam)
28. [2025 ICCV] **Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation**[[paper]](https://kimsure.github.io/images/files/iccv25_sfp_full.pdf)[[code]](https://github.com/Kimsure/SFP)
29. [2025 ICCV] **Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation**[[paper]](https://arxiv.org/pdf/2508.20265)
30. [2025 ICCV] **Test-Time Retrieval-Augmented Adaptation for Vision-Language Models**[[code]](https://github.com/xinqi-fan/TT-RAA)
31. [2025 ICCV] **ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction**[[paper]](https://arxiv.org/pdf/2507.15803)
32. [2025 ICCV] **Text-guided Visual Prompt DINO for Generic Segmentation**[[paper]](https://arxiv.org/pdf/2508.06146)[[code]](https://github.com/WeChatCV/WeVisionOne)
33. [2025 ICCV] **SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation**[[paper]](https://arxiv.org/pdf/2507.12857)[[code]](https://github.com/HuangShiqi128/SCORE)
34. [2025 ICCV] **Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation**
35. [2025 arXiv] **Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation** [[paper]](https://arxiv.org/pdf/2411.15869) [[code]](https://github.com/SuleBai/SC-CLIP?tab=readme-ov-file)
36. [2025 arXiv] **Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2505.21844v1) [[code]](https://github.com/dosowiechi/MLMP?tab=readme-ov-file)
37. [2025 arXiv] **FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/abs/2504.10487) [[code]](https://github.com/yasserben/FLOSS)
38. [2025 arXiv] **TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models** [[paper]](https://arxiv.org/pdf/2505.23769) [[code]](https://github.com/avaxiao/TextRegion)
39. [2025 arXiv] **A Survey on Training-free Open-Vocabulary Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2505.22209)

### è€å¸ˆæä¾›æš‚å­˜ï¼š
1. [2025 arXiv] **POT: Prototypical Optimal Transport for Weakly Supervised Semantic Segmentation** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)
> å¼±ç›‘ç£ç—…ç†å›¾åƒçš„è¯­ä¹‰åˆ†å‰²
2. [2025CVPR] **Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)
> å¼±ç›‘ç£ç—…ç†å›¾åƒçš„è¯­ä¹‰åˆ†å‰²
3. [2024 arXiv]**Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP** [[paper]](https://arxiv.org/pdf/2412.19650)
4. [2025CVPR]**Prompt Categories Cluster for Weakly Supervised Semantic Segmentation** [[paper]](https://openaccess.thecvf.com/content/CVPR2025W/eLVM/papers/Wu_Prompt_Categories_Cluster_for_Weakly_Supervised_Semantic_Segmentation_CVPRW_2025_paper.pdf)
5. [2025 arXiv]  **2025-NIPS-Disentangling CLIP for Multi-Object Perception** [[paper]](https://arxiv.org/html/2502.02977v3)
6. [2021 ICLR] **A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention**[[paper]](https://arxiv.org/pdf/2006.12065)
7.[2025 ICCV] **Interpretable point cloud classification using multiple instance learning**[[paper]]()


### 3Dç‚¹äº‘å¤„ç†å’Œè§†è§‰-è¯­è¨€æ¨¡å‹
1. [2025-ICCV] **Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation**
2. [2025-ICCV] **Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval**
3. [2025-ICCV] **Partially Matching Submap Helps: Uncertainty Modeling and Propagation for Text to Point Cloud Localization**
4. [2025-ICCV] **Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds**
5. [2025-CVPR] **Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model**
6. [2025-arXiv] **Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation**  [[paper]](https://arxiv.org/html/2506.22375v1)
7. [2025-ICLR] **MULTIMODALITY HELPS FEW-SHOT 3D POINT CLOUD SEMANTIC SEGMENTATION** [[paper]](https://arxiv.org/pdf/2410.11414)
8. [2025-ICML] **SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds**
9. [2025-CVPR] **Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis**
10. [2025-CVPR] **Purge-Gate: Efficient Backpropagation-Free Test-Time Adaptation for Point Clouds via Token Purging**
11. [2025-ICCV] **Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval**

